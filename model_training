### model training

构建dataset,根据自己的需求定义
dataloader
构建模型(继承module)

超参数：
lr,bs,epoch

损失函数:
交叉熵，

from torch.optim import optim #导入优化器模块
优化器：
torch.optim.SGD(model.parameters(),lr=learning_rate)
optimizer.zero_grad()#每次训练之前得执行该操作，梯度置零
loss.backwards()#计算梯度
optimizer.step()模型更新

train_loop

test_loop

预测时不用优化参数，只需要改batch_size，在gpu或cpu预测
