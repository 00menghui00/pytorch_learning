### model training

构建dataset,根据自己的需求定义
dataloader
构建模型(继承module)

超参数：
lr,bs,epoch

损失函数:
交叉熵，

from torch.optim import optim #导入优化器模块
优化器：
torch.optim.SGD(model.parameters(),lr=learning_rate)
optimizer.zero_grad()#每次训练之前得执行该操作，梯度置零
loss.backwards()#计算梯度
optimizer.step()模型更新

train_loop

test_loop

预测时不用优化参数，只需要改batch_size，在gpu或cpu预测

###
关于前向传播函数：
在 PyTorch 中，你通过 model(input) 的方式来执行模型的前向传播，而不需要显式地调用 model.forward(input)。这是通过 nn.Module 的 __call__ 方法实现的，它会自动调用你定义的 forward() 方法。这种设计使得代码更加简洁和易读。
###
model.train() 
是 PyTorch 中一个非常重要的函数，用于将模型设置为训练模式。这个函数主要影响模型中特定层的行为，尤其是那些在训练和评估阶段行为不同的层，例如：

Batch Normalization (BN) 层： 在训练过程中，BN 层使用当前批次的均值和方差来规范化激活值。而在评估（或推理）过程中，BN 层使用在训练期间计算的全局均值和方差。
Dropout 层： 在训练过程中，Dropout 层会随机地将一部分神经元的输出置零，以防止过拟合。而在评估过程中，Dropout 层不执行任何操作（相当于所有神经元的保留概率为 1）。
###
