
torch.utils.data.Dataset:对单样本而言,每次只处理一个样本

torch.utils.data.DataLoader:对多个样本而言，对样本批量处理

构建自己的Dataset类
	继承Dataset:
def __init__(self,annotations_file,img_dir,transform=None,target_transform=None):
	传入文件路径
	self.sequence_length:不是官方自带的，用于获取序列的长度，一般用于NLP或时间序列数据
	self.transform:对图像数据进行预处理/数据增强
def __len__(self):
	返回数据的大小
def __getitem__ (self,index)：
	基于索引返回训练样本(image,label),transform:数据预处理

DataLoader：批量样本，打乱原顺序(避免过拟合)，多线程
DataLoader(dataset,batch_size,shuffle(打乱),sampler(采样),num_workers(线程),pin_memory(tensor保存在gpu中),collate_fn(对batch_size进行处理))

将dataset导入dataloader后可以对dataset遍历

DataLoader源码详细讲解：
__init__(self,dataset,batch_size,shuffle,sampler(自定义样本采样方法),batch_sampler,num_workers,collate_fn)
	sample(自定义采样)与shuffle(默认采样方法)只能用一个
	batch_sample与batch_size只能用一个,shuffle与sampler也不用了

RandomSampler
__iter__(self):获取数据集长度，
	torch.randperm:返回0到n-1的列表的随机组合

SequentialSampler
__iter__(self):按某种顺序采样
	batch_sampler：从dataset中以sampler拼接的元素以index返回
	drop_last:丢弃最后散的数据

_index_sampler

_next_data

transform
target_transform

###
train_dataset = TensorDataset(X_train, y_train):

TensorDataset 是 PyTorch 的一个类，用于将张量组合成数据集。它接收多个张量作为输入，并将它们按索引进行配对。	

###
train_dataloader = DataLoader(train_dataset, batch_size=64, shuffle=True):

DataLoader 是 PyTorch 的另一个类，用于创建数据加载器。数据加载器负责将数据集划分为小批次（batch），并提供迭代器来遍历这些批次。
DataLoader(train_dataset, batch_size=64, shuffle=True) 创建了一个训练数据加载器 train_dataloader，参数解释如下：
train_dataset: 要加载的数据集，即上面创建的 train_dataset。
batch_size=64: 每个批次包含的样本数量。这意味着 train_dataloader 每次迭代将返回 64 个样本。
shuffle=True: 是否在每个 epoch 开始时打乱数据。对于训练数据，通常需要打乱数据，以避免模型在训练过程中学习到数据的顺序。
